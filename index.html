<!DOCTYPE html>
<html>
  <head>
    <link href='http://fonts.googleapis.com/css?family=Raleway:400,100,700' rel='stylesheet' type='text/css'>
    <link href="prism.css" rel="stylesheet" />
    <link href="style.css" rel="stylesheet" />
    <script src="http://code.jquery.com/jquery-latest.js"></script>
    <script src="jquery.smooth-scroll.min.js"></script>
    <script src="prism.js"></script>
    <script>
    $(window).load(function() { 
       $('#start').css('min-height', window.innerHeight); 
       $('pre, code').attr('class', 'language-terra');    
       Prism.highlightAll();
       $('#links a').smoothScroll();                 
     });
    </script>
  </head>
  <body>
    <div id="start">
      <div id="logo">
        <h1>terracuda</h1>
        <div id="links">
          <a href="#proposal">proposal</a>
          <span>&#12297;</span>
          <a href="#checkpoint">checkpoint</a>
          <span>&#12297;</span>
          <a href="#writeup">writeup</a>
        </div>
      </div>
    </div>
    <div id="content">

      <h1 id="writeup">Writeup</h1>

      <h2>Summary</h2>
      <p>We created a high-level Lua/Terra API which uses CUDA to execute Terra functions on the GPU. Our API can translate arbitrary Terra code to CUDA code and handles all device memory management and similar issues behind the scenes.</p>

      <h2>Background</h2>

      <p>Writing GPU code is time-consuming. CUDA, the best available language for GPU programming, requires a lot of work from the programmer in order to do even simple tasks. The programmer must remember to keep host memory and device memory in sync. He must calculate the number of threads and warps he needs (and the term "warp" is quite confusing if you haven't heard it before). He must calculate for himself the appropriate index at each instance of his kernel. And most importantly, he can only write GPU code in CUDA!</p>
      <p>Some APIs like <a href="http://documen.tician.de/pycuda/">PyCUDA</a> and <a href="http://www.jcuda.org/samples/samples.html">JCuda</a> attempted to tackle this problem. Unfortunately, they only address the last point raised above&mdash;one can use CUDA functions in the course of writing a Python program, but it is neither easy nor intuitive. In PyCUDA, for example, one must actually embed CUDA code as a string inside of the Python program (or presumably load it from a file). Terracuda's goal was to abstract away all these issues and let the programmer write GPU code at his convenience.</p>


      <h2>Approach</h2>
      <p><a href="http://www.lua.org/">Lua</a> is a fast, lightweight, and embeddable scripting language found in places like Wikipedia, World of Warcraft, Photoshop Lightroom, and more. Lua's simple syntax and dynamic typing also make it an ideal language for novice programmers. <a href="http://terralang.org">Terra</a> is a language embedded in Lua which gives easier access to low-level functions (e.g. printf).</p>

      <p>Terra has a few key features we used: it JIT-compiles to LLVM, it interoperates with Lua, and its types are treated as values in Lua. This combined with the recent release of NVIDIA's <a href="http://llvm.org/docs/NVPTXUsage.html">NVPTX</a> LLVM backend means that we can easily translate between Terra code and CUDA code. Its interoperation means that any Lua value can be used in Terra code, and its type system means that Lua can generate dynamics types on the fly.</p>
      <p>Before we delve into the specifics of the API, let's look at a sample implementation of the <code>SAXPY</code> routine in regular Lua and using Terracuda.</p>
      <div style="overflow: auto; margin-top: 20px;">
        <div style="float:left;">
          <strong>Lua</strong>
          <pre style="margin-top: 5px!important; width: 360px;"><code>local P, N = {X = {}, Y = {}, a = 2.0}, 1000
-- populate X

for i = 1, N do
   P.Y[i] = P.A * P.X[i] + P.Y[i]
end</code></pre>
        </div>
        <div style="float:left; margin-left: 20px;">
          <strong>Terracuda</strong>
          <pre style="margin-top: 5px!important; width: 360px;"><code>local P, N = {X = {}, Y = {}, a = 2.0}, 1000
-- populate X

local ptype = cudalib.make_struct_type(P)
local saxpy = cudalib.make_kernel(
terra(p : &ptype, i : int)
   p.Y[i] = p.a * p.X[i] + p.Y[i]
end)

saxpy(P, N)</code></pre>
        </div>
      </div>
      
      <p>Both pieces of code above will produce the same output (i.e. <code>P.C</code> will contain the correct values). You may notice, however, that device memory, thread indexing, kernel calls, etc. are all conspicuously absent from the Terracuda code. This is all handled by the API. Here's how it works: consider that you have some table <code>P</code> a Terra function <code>F</code> that operates on <code>P</code> which you want to convert to GPU code. The process is as follows:</p>
      <ol>
        <li>Run <code>make_struct_type(P)</code>. Because Terra types are values, Lua can loop through <code>P</code> and generate a new <code>struct</code> type that corresponds exactly with the given input. So in the above <code>SAXPY</code> example, the generated type is <code>struct P { X: &int, Y: &int, a: double }</code>.</li>
        <li>Run <code>make_kernel(F)</code>. This pre-allocates device memory for every entry inside the struct if possible (the exception being if we have variable length arrays). <code>make_kernel</code> returns a closure over these values which will copy the input into the allocated device memory, run the kernel on the device parameters, and copy the modified memory back to the host. This routine also compiles <code>F</code> to GPU code for later use.</li>
        <li>Run the result of <code>make_kernel</code> with your input parameters as well as the number of threads <code>N</code> to create.
      </ol>
      
      <p>That's the basics of the API. I'll add more here soon on how each iteration of the API changed.</p>

      <h2>Results</h2>
      <p>The analysis is promising (and still ongoing!). The API itself works great, so we concerned ourselves mostly with performance and concision. We wrote a myriad of applications in C, CUDA, Terra, and Terracuda ranging from <code>SAXPY</code> to a <a href="http://15418.courses.cs.cmu.edu/spring2014/article/4">circle renderer</a> we wrote for class. For each application, we sampled execution time and also recorded the size of the codebase required for the CUDA implementation versus the Terracuda one. We found the following execution times:</p>
      <img src="graph1.png" width="600"  />
      <p>So far, Terracuda (despite being JIT-compiled in a dynamically typed language) is mostly competitive with C and CUDA. In some cases, like with matrix multiplication, it actually performs better than hand-written CUDA! It doesn't work as well, however, on simple programs like <code>SAXPY</code> as the overhead of compiling/spinning up a kernel is greater than any performance gains.</p>
      <p>Code base comparison also yielded favorable results. Terracuda was consistently less than <code>1/2</code> the size of its CUDA counterpart, which emphasizes the concise nature of both Lua and the API we've created. Granted, lines of code is not a perfect benchmark for good/bad code (since it also includes comments/includes/etc.), but it gives us a good impression of relative code sizes. See the results:</p>

      <img src="graph2.png" width="600"  />

      <h2>References</h2>
      <p>DeVito, Zachary et al. <em>Terra: A Multi-Stage Language for High-Performance Computing</em>. 2013.</p>
      
      <h2>Work done</h2>
      <p>Equal work was performed by both project members.</p>

      <h1 id="checkpoint">Checkpoint</h1>

      <p>The project is proceeding mostly according to plan. Here's the highlights:</p>

      <ul>
        <li>Because of some troubles with installation/setup/learning overhead, the API is not as close to completion as it should be. That said, as you can see in the Github repo we have made significant progress in creating a high-level CUDA API. We have multiple flavors of a map primitive (which is the basic underpinning of any CUDA program). Soon we'll have support for managing multiple variables/kernels within a CUDA program instead of a single list. </li>
        <li><p>We've benchmarked a basic program and seen some cool performance gains. We ran the following code over an array length 100k</p>

          <pre><code>terra do_work(x : int) : int
   var y : int = 0
   for i = 0, 10000 do
      if i % 3 == 0 or i % 5 == 0 then
         y = y + 1
      end
   end
   return x + y
end</code></pre>

          <p>And we found speedups of 7.6x over JIT'd Terra, 14.34x over native C, and within 0.95x of hand-rolled CUDA. And the mapping API is really easy to use, it just looks like:</p>

          <pre><code>local kernel = cuda.lua_map(do_work)
kernel(some_list)</code></pre>

          <p>And some_list gets modified in-place. </p></li>
          <li>After a lot of blood and tears, we managed to port the serial and CUDA circle renderers to Lua (so we are ahead of schedule in that respect!). We still need to implement the optimizations from asst2 (e.g. quadtree data structure on the circles) and change it to use more of our newly minted API, but that should come quickly in the next few weeks. For the competition, we'll show off the circle renderer and any other examples we may come up with before then. We'll also have graphs comparing speed and code size using our API. </li>
          <li>At this point, most of the work is just programming and any further API design as our needs develop. We're not facing any serious issues at this point. </li>
      </ul>

      <h1 id="proposal">Proposal</h1>

      <h2>Summary</h2>

      <p>We will create a CUDA API for Lua aimed at programmers unfamiliar with GPU-level parallelism.</p>

      <h2>Background</h2>

      <p><a href="http://www.lua.org/">Lua</a> is a fast, lightweight, and embeddable scripting language found in places like Wikipedia, World of Warcraft, Photoshop Lightroom, and more. Lua's simple syntax and dynamic typing also make it an ideal language for novice programmers. Traditionally, languages like Lua find themselves abstracted miles above low-level parallel frameworks like <a href="http://www.nvidia.com/object/cuda_home_new.html">CUDA</a>, and consequently GPU parallelism was limited to programmers using a systems language like C++. Frameworks like <a href="http://terralang.org/">Terra</a>, however, work to close that gap, making low-level programming accessible in a high-level interface. However, these interfaces still require a number of calls to C libraries and intimate knowledge of the CUDA library. For example, the following code runs a simple CUDA kernel in Terra:</p>

      <pre><code>terra foo(result : &amp;float)
    var t = tid()
    result[t] = t
end

local R = terralib.cudacompile({ bar = foo })

terra run_cuda_code(N : int)
    var data : &amp;float
    C.cudaMalloc([&amp;&amp;opaque](&amp;data),sizeof(float)*N)
    var launch = terralib.CUDAParams { 1,1,1, N,1,1, 0, nil }
    R.bar(&amp;launch,data)
    var results : &amp;float = [&amp;float](C.malloc(sizeof(float)*N))
    C.cudaMemcpy(results,data,sizeof(float)*N,2)
    return results;
end

results = run_cuda_code(16)</code></pre>

      <p>Other high-level CUDA bindings like <a href="http://documen.tician.de/pycuda/">PyCUDA</a> and <a href="http://www.jcuda.org/samples/samples.html">JCuda</a> suffer the same problem.</p>

      <h2>The Challenge</h2>

      <p>The problem is challenging foremost on the level of architecture. Designing an API is never easy, and attempting to expose GPU-level parallelism to a language as high-level as Lua requires a great deal of care to be usable while still being useful. Creating such an API requires significant knowledge of the abstraction layers between Lua, C, and CUDA as well as knowledge of the typical use cases for high-level parallelism. </p>

      <p>My partner and I know neither Terra nor LLVM (which Terra compiles to), so creating these high-level bindings requires a great deal of initial investment. The existing interface between Terra and CUDA is sketchy at best, so we will need to implement significant new functionality into Terra in order for the Circle Renderer to function properly.</p>

      <h2>Resources</h2>

      <p>For machines, we'll just be using any computers equipped with NVIDIA GPUs (i.e. Will's laptop and the Gates 5k machines). No other special hardware/software will be needed. We'll be building upon the Terra language and also using <a href="http://luagl.sourceforge.net/">LuaGL</a> for some of the demos.</p>

      <h2>Goals</h2>

      <p>The project has three main areas: writing the API, creating programs using the API, and benchmarking the code against other languages/compilers.</p>

      <p>We plan to achieve:</p>

      <ul>
        <li><strong>Writing the API</strong>
          <ul><li>Allow arbitrary Lua code to be executed in the GPU over a table.</li>
            <li>Optimize threads/warp usage to the input data.</li>
            <li>Abstract the API such that the user needs no C libraries and as little Terra as possible.</li></ul></li>
            <li><strong>Creating programs</strong>
              <ul><li>Make a simple saxpy</li>
                <li>Write matrix operations like transpose or pseudoinverse/SVD</li>
                <li>Port the Assignment 2 Circle Renderer over to vanilla Lua (using LuaGL)</li></ul></li>
                <li><strong>Benchmarking</strong>
                  <ul><li>For each program, benchmark it against equivalent implementations in: vanilla Lua, Terra without CUDA, and C.</li></ul></li>
      </ul>

      <p>We hope to achieve:</p>

      <ul>
        <li>Achieve better performance than vanilla C.</li>
        <li>Implement shared memory in Terra.</li>
        <li>Implement linking against libraries like <a href="https://developer.nvidia.com/cublas">cublas</a>.</li>
      </ul>

      <h2>Platform</h2>

      <p>CUDA makes sense as we've already learned it in class, and Lua makes sense as Terra already laid the foundation for abstracting systems-level code.</p>

      <h2>Schedule</h2>

      <h3>Original schedule</h3>

      <ul>
        <li><strong>Friday, April 11</strong>: finish map primitives (ie any Lua code and map over a Lua table in CUDA). Write saxpy and corresponding benchmarks.</li>
        <li><strong>Friday, April 18</strong>: complete Terracuda API. Write matrix code and benchmarks.</li>
        <li><strong>Friday, April 25</strong>: Port over Circle Renderer and benchmarks. Gather all requisite data and perform preliminary analysis.</li>
        <li><strong>Friday, May 2</strong>: Optimize/refactor API based on code written and data found. Search for possible performance gains in the abstraction layer. Attempt to implement library linking.</li>
        <li><strong>Friday, May 9</strong>: create writeup based on finalized API. Add any remaining features, time permitting (.g. shared memory).</li>
      </ul>

      <h3>New schedule</h3>

      <ul>
        <li><strong>Wednesday, April 23</strong>: implement generic kernel/variable and reduce primitives (Will).</li>
        <li><strong>Sunday, April 27</strong>: finish implementing optimizations in circle renderer (Patrick).</li>
        <li><strong>Wednesday, April 30</strong>: finish refactors to API. Add shared/constant memory if possible (Will).</li>
        <li><strong>Sunday, May 3</strong>: finish benchmarking all programs. Gather and graph data. (Patrick).</li>
        <li><strong>Wednesday, May 7</strong>: complete write-up (Will and Patrick). Time permitting, make more demos.</li>
      </ul>

    </div>
  </body>
</html>